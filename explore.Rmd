---
title: "Explore Twitter Data"
author: "Lyn Nguyen"
date: '2022-12-07'
output:
  pdf_document: 
    toc: true
    toc_depth: 4
  html_document: 
    toc: true
    toc_depth: 4
    number_sections: true
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F)
library(dplyr)
library(lubridate)
library(stringr)
library(ggplot2)
```
# ELT

This script uses output from analysis-of-public-opinion/scraper.py. Ultimately, we keep data pulled on Dec
```{r echo=FALSE}
tweets1 <- read.csv("prelim_data/tweets1.csv")
tweets2 <- read.csv("prelim_data/tweets2.csv")
tweets3 <- read.csv("prelim_data/tweets3.csv") # 467 rows
tweets4 <- read.csv("prelim_data/tweets4.csv") # 347 rows 

users3 <- read.csv("prelim_data/users3.csv") # 467 rows
users4 <- read.csv("prelim_data/users4.csv") # 347 rows 

```


```{r}
# created_at to date and day of week 
test = head(tweets1) 
dow <- substr(test$created_at, 1, 3)
month_day <- substr(test$created_at, 5, 10)
time<- substr(test$created_at, 12, 19)
yr <- substr(test$created_at, 26, 30)

ymd <- as.Date(paste0(month_day, yr), format = "%b %d %h:%m:%s %Y")

# as.Date(test$created_at, format = "%a %b %d %h:%m:%s +0000 %Y")
tweets1 <- tweets1 %>% mutate(dow = substr(created_at, 1, 3)
                              , month_day = substr(created_at, 5, 10)
                              , time = substr(created_at, 12, 19)
                              , yr = substr(created_at, 26, 30), 
                              , ymd = as.Date(paste0(month_day, yr), format = "%b %d %Y"))
tweets2 <- tweets2 %>% mutate(dow = substr(created_at, 1, 3)
                              , month_day = substr(created_at, 5, 10)
                              , time = substr(created_at, 12, 19)
                              , yr = substr(created_at, 26, 30), 
                              , ymd = as.Date(paste0(month_day, yr), format = "%b %d %Y"))
tweets3 <- tweets3 %>% mutate(dow = substr(created_at, 1, 3)
                              , month_day = substr(created_at, 5, 10)
                              , time = substr(created_at, 12, 19)
                              , yr = substr(created_at, 26, 30), 
                              , ymd = as.Date(paste0(month_day, yr), format = "%b %d %Y")
                              , tweet_id_char = as.character(as.numeric(tweet_id)))
tweets4 <- tweets4 %>% mutate(dow = substr(created_at, 1, 3)
                              , month_day = substr(created_at, 5, 10)
                              , time = substr(created_at, 12, 19)
                              , yr = substr(created_at, 26, 30), 
                              , ymd = as.Date(paste0(month_day, yr), format = "%b %d %Y")
                              , tweet_id_char = as.character(as.numeric(tweet_id)))
summary(tweets1$ymd) 
summary(tweets2$ymd)
summary(tweets3$ymd)
summary(tweets4$ymd)

```

`tweets1.csv` has data from 11/26/2022 but only cnn as liberal source. 
`tweet2.csv`: 11/26- 12/3 but only cnn as liberal source
`tweet3.csv`: 11/28- 12/3 liberal sources has cnn, npr, msnbc, nytimes, 
`tweet4.csv`: 11/28- 12/3 but only cnn as liberal source

`tweets1` and `tweets2` have 814 fields total, but only 468 unique. 

```{r}
master <- rbind(tweets3, tweets4) %>% select(-experiment_id) %>% distinct()
```

`master` has 471 points, but `length(unique(master$tweet_id))` has 468 points. Where is the 3 difference? Since tweet4 hit the api after tweet3, some has updated values. For example tweet_id "1598304394931412992" has 0 like in tweet3 but 1 like in tweet 4. If there is duplicate in tweet_id, we will keep the one with the higher index. 

```{r}
master <- master %>% mutate(tweet_id_char = as.character(as.numeric(tweet_id)))
master_tweet_id <- master$tweet_id_char
dup_master <- master_tweet_id[duplicated(master_tweet_id) == T] 

print("The duplicated tweet_ids are:")
dup_master
```
3 tweets are duplicated because they have updated "likes" count. 
```{r}
dup_val1 <- master[master$tweet_id == 1598304394931412992, ][2,]
dup_val2 <- master[master$tweet_id == 1598277959223083008, ][2,]
dup_val3 <- master[master$tweet_id == 1598411537667874816, ][2,]

m <- master %>% filter(!tweet_id %in% dup_master)
master <- rbind(m, dup_val1, dup_val2, dup_val3) %>% arrange(tweet_id) # in ascending tweet_id order 

# write.csv(master, "prelim_data/tweets_master_dec5dec6.csv")
```

## Get `text` and `tweet_id` only. 
Madelaine will use this file in SageMaker. Need to keep row orders for annotation output. 
```{r}
tweet_text <- master %>% select("tweet_id", "text") %>% distinct() #468 
# write.csv(tweet_text, "prelim_data/tweet_text_only.csv")
```


## Clean up `users`.
```{r}
# What user_id in user3 that's not in user4? 
user4_id <- users4$user_id
user3_id <- users3$user_id

user3_id_only <- setdiff(user3_id, user4_id) #ids in user3 that's not in user4 - 118 total 
user3_profiles_only <- users3 %>% filter(user_id %in% user3_id_only)

all_users <- rbind(user3_profiles_only, users4) %>% mutate(user_id_char = as.character(as.numeric(user_id))) %>% distinct() # 459 total users 

# merge users and tweets ===
# rename overlap column names 
tweet_cnames <- colnames(master)
colnames(master) <- c("experiment_group", "text", "tweet_id", "tweet_likes", "retweets","tweet_created_at","user_id","in_reply_to_status_id","in_reply_to_user_id", 
                      "in_reply_to_screen_name", "screen_name","dow","month_day","time","yr","ymd","tweet_id_char")
author_cnames <- colnames(all_users)

final_tweets <- left_join(master %>% select(-c(screen_name)), all_users, by = "user_id")

write.csv(final_tweets, "data/master.csv")
```

There are `r nrow(all_users)` unique authors for these 468 tweets.

\newpage
# EDA

`final_tweets` have 25 columns and 468 observations (tweets). 
```{r}
glimpse(final_tweets)
```

## experiment_group / in_reply_to_screen_name

*What is the share of replies to the 5 news sources? How do ('msnbc', 'cnn', 'npr', 'nytimes') compare to 'cnn'?*
- FoxNews make up 87% of our data points. When it comes to the student loan forgiveness discussion, the Department of Education has the least engagement from Twitter users, at only 1%. 

```{r}
liberal <- c('msnbc', 'cnn', 'npr', 'nytimes')
conservative <- c('foxnews')

source_count <- as.data.frame(table(final_tweets$in_reply_to_screen_name)) %>% mutate(Proportion = round(Freq/nrow(final_tweets), 2)) %>% arrange(Freq)
source_count
barplot(source_count$Freq)
```

## Tweet 

### `text`

*Is tweet length a distinguishable characteristic for the experiment groups?*
Within the liberal groups, most of NPR replies have over 40 words. NYTimes's reply lengths are scattered on the lower end.  
```{r}
final_tweets <- final_tweets %>% mutate(text_length = nchar(text), 
                                      text_word_count = str_count(text, '\\w+'))


l <- final_tweets %>% filter(experiment_group %in% liberal) %>% 
  select(experiment_group, text_length, text_word_count)

colors <- c("#FDAE61", # Orange
            "#D9EF8B", # Light green
            "#66BD63") # Darker green
x <- l$text_word_count
y <- l$text_length
group <- l$experiment_group
# Scatter plot
ggplot(l, aes(x, y, color = factor(group))) + geom_point(size = 2) + xlab("Word Count") + ylab("Text Length")

```

```{r}
x <- final_tweets$text_word_count
y <- final_tweets$text_length
group <- final_tweets$experiment_group
ggplot(final_tweets, aes(x, y, color = factor(group))) + geom_point(size = 2) + xlab("Word Count") + ylab("Text Length")

```

```{r}
# how does nchar treat emojis? - no. 
test = final_tweets[463,] %>% select("text", "text_word_count", "text_length")

```


### Tweet Popularity


#### `retweets` & `experiment_group`
*Which tweet has more `retweets`? Does it happen more often on liberal or conservative outlet?*
One tweet has 85 retweets, one have 5 retweets, three have 3 retweets but the majority 447 (96%) do not have any retweets.


```{r}
retweet_count <- data.frame(table(final_tweets$retweets)) %>% arrange(desc(Freq))
colnames(retweet_count) <- c("retweets", "freq")
retweet_count
```

*Which outlet has posts with more than 1 retweet? *
Foxnews and NPR are the two sources where replies have over 1 retweet, with Foxnews holding the highest, 85 retweets.

```{r}
many_retweets <- final_tweets %>% filter(retweets > 1) %>% select(experiment_group, retweets, screen_name)
many_retweets
```

#### `tweet_likes` & `experiment_group`
*Which tweet has more `likes`? Does it happen more often on liberal or conservative outlet?*
One post has 5446 likes, but the majority (308 out of 478) have 0 likes. 
```{r}
tweet_like_count <- data.frame(table(final_tweets$tweet_likes)) %>% arrange(desc(Freq))
colnames(tweet_like_count) <- c("likes_count", "freq")
```

*Where is the highest retweet reply?*
- A tweet addressing FoxNews from someone who is against student loan forgiveness.
```{r}
print(final_tweets[which.max(final_tweets$tweet_likes),]$experiment_group)
print(final_tweets[which.max(final_tweets$tweet_likes),]$text)
print(final_tweets[which.max(final_tweets$tweet_likes),]$tweet_likes)
```


*On average, does conservative or liberal sources have more likes and retweets? (after discounting the post with 5446)*
- NPR has the most average likes and average retweets out of all 5 sources. Replies to Foxnews are 3rd from the bottom in average tweets, even though 87% of the replies in the population belongs to them. On average its replies stand 2nd to last, beating USEdGov, who has less than 1 like on average. 

```{r}
no_max_likes <- final_tweets %>% filter(tweet_likes != 5446)
no_max_likes <- no_max_likes %>% group_by(experiment_group) %>% 
  summarize(avg_likes = mean(tweet_likes), agg_likes = sum(tweet_likes), 
            avg_retweets = mean(retweets), agg_retweets = sum(retweets))
no_max_likes
```

- When combining the liberal sources, the liberal sources on average have 30% more likes and has 6 times more average retweets than the conservative foxnews. 

```{r}
no_max_likes <- final_tweets %>% filter(tweet_likes != 5446) %>% 
  mutate(politics = ifelse(experiment_group %in% c('cnn', 'msnbc', 'npr', 'nytimes'), 'liberal', 
                           ifelse(experiment_group == 'usedgov', 'controlled', 'conservative')))
no_max_likes <- no_max_likes %>% group_by(politics) %>% 
  summarize(avg_likes = mean(tweet_likes), agg_likes = sum(tweet_likes), 
            avg_retweets = mean(retweets), agg_retweets = sum(retweets))
no_max_likes
```


## User 

### `screen_name`

*1. Which author has multiple replies? Do they reply to the same source or not?* 

- 8 people replied twice, 2 of which to multiple news source twitters, but only 1 engage with conservative (FoxNews) and liberal (MSNBC).
```{r}
author_multtweet <- c(data.frame(table(final_tweets$screen_name)) %>% filter(Freq > 1) %>% select(Var1))

author_overlap <- final_tweets %>% filter(screen_name %in% c("DahlmanCarl", "fabulosi_t", "jackSpa81774793", "johnbutler410", "michael_favreau", "PCopposition", "RogerWPetersen1", "thomaslew13" )) %>% select(in_reply_to_screen_name, screen_name, statuses_count, favourites_count, followers_count, tweet_likes, retweets)

author_overlap 
```

### `created_at`
*Does age of account tell who they might engage with?*
```{r}
today <- as.Date("2022-12-08")

ft <- final_tweets %>% mutate(age_dow = substr(created_at, 1, 3)
                              , age_month_day = substr(created_at, 5, 10)
                              , age_time = substr(created_at, 12, 19)
                              , age_yr = substr(created_at, 26, 30), 
                              , age_ymd = as.Date(paste0(age_month_day, age_yr), format = "%b %d %Y"),
                              , account_age = today - age_ymd)

today <- as.Date("2022-12-08")
ft <- ft %>% mutate(account_age = today - age_ymd)

print(paste("min age of acct (days): ", min(ft$account_age)))
print(paste("max age of acct (days): ", max(ft$account_age)))
print(paste("mean age of acct (days): ", mean(ft$account_age)))
print(paste("median age of acct (days): ", median(ft$account_age)))

```

The youngest `account_age` is 5 days, and the oldest account is 14 years (5257 days)

```{r}
plot(ft$account_age)
```
While most accounts are under 3 years old, there are a handful of accounts in the 4000-5000 days range. Let's look at the text of the accounts with more than 5000 days in age. 6 accounts are over 5000 days old. Majority of them are critical to student loan forgiveness.

One text https://twitter.com/jack_jackson/status/1598689928946323458 @ both NPR and FoxNews. However, the text is an original text (`in_reply_to_status_id` is N/A). Maybe it's okay to keep the `experiment_group` as NPR since mentioning them first prioritize them over Foxnews? 

```{r}
ft %>% filter(account_age > 5000) %>% select(experiment_group, text)
```

### `description`
*How many have profile descriptions?*
More than half of the tweeters don't have an account profile description. Are the share of those with and without description proportional based on who they reply to? 
```{r}
(final_tweets %>% mutate(has_profile_desc = ifelse(nchar(description) == 0, 0, 1)) )%>% group_by(has_profile_desc) %>% summarize(agg_profile_desc = n())
```

### `location`
*How many have profile location display? Is one location more dense?* 

Most of the tweets belong to tweeter with no locations (66%).

```{r}
(final_tweets %>% mutate(has_profile_loc= ifelse(nchar(location) == 0, 0, 1)) )%>% group_by(has_profile_loc) %>% summarize(agg_profile_loc = n())
```

### `ymd` & `dow`
*Which day of the week do people discuss student loan forgiveness the most often?*
Recall that data is from Sunday 11/27 - Tuesday 12/6Thursdays and Saturdays get the most tweets.
```{r}
final_tweets %>% group_by(dow) %>% summarize(tweet_count = n())
```



*bar plot of tweet count by day*
5 of 9 days have fewer than 20 tweets. Thursday Dec. 1st makes up 33% of all tweets. On Dec 1st, Supreme Court announced they will expedite the process. 
- https://www.nytimes.com/2022/12/01/us/politics/supreme-court-student-loan-forgiveness.html
- https://www.washingtonpost.com/politics/2022/12/01/supreme-court-review-student-loan-forgiveness/

```{r}
ymd_data <- final_tweets %>% group_by(ymd) %>% summarize(tweet_count = n())

barplot(height = ymd_data$tweet_count, names = (ymd_data$ymd))
```
### `time`

*What time of day has the most discussion?*

```{r}
time <- final_tweets %>% mutate(hour = substr(time, 1, 2))
time_gr <- time %>% group_by(hour) %>% summarize(freq_by_hr = n())
time_gr
```

*bar plot of tweet count by hour*
Tweets on this topic lulls between 8-10 am. The afternoon has the highest engagement, with a decrease before commuting time, and an rise right after. 
```{r}
barplot(height = time_gr$freq_by_hr, names = (time_gr$hour))
```

*bar plot with multiple colors for conservative vs. liberal*

8pm is a popular time for engagement within our control and liberal groups. 
```{r}
final_tweets <- final_tweets %>% 
  mutate(hour = substr(time, 1, 2), 
         politics = ifelse(experiment_group %in% c('cnn', 'msnbc', 'npr', 'nytimes'), 'liberal', 
                           ifelse(experiment_group == 'usedgov', 'controlled', 'conservative')))
stacked_time <- final_tweets %>% group_by(politics, hour) %>% summarize(Freq = n())

ggplot(stacked_time, aes(fill=politics, y=Freq, x=hour)) + 
    geom_bar(position="stack", stat="identity")
```

*Is the 8pm mostly due to the Supreme Court announcement on Dec.1st?*
Yes, Dec 1st makes up over 50% of total tweets at 8pm. 
```{r}
dec1_stacked_time <- final_tweets %>% filter(ymd == "2022-12-01") %>% 
  group_by(politics, hour) %>% summarize(Freq = n())

ggplot(dec1_stacked_time, aes(fill=politics, y=Freq, x=hour)) + 
    geom_bar(position="stack", stat="identity")
```


### User Popularity 

#### `favourites_count` & `followers_count`

**Which media sources has engagement from the most favorite tweeter?**

Tweeters engaging with liberal news source had 5 times more profile favorites and 3.6 times more followers, on average. Although fewer tweets addressed the controlled sources, they have more followers on average than accounts engaging in both conservative and liberal media. 

```{r}
fav_counts_tweet <- data.frame(table(final_tweets$favourites_count)) %>% arrange(desc(Freq))

fc <- final_tweets %>% #filter(tweet_likes != 5446) %>% 
  mutate(politics = ifelse(experiment_group %in% c('cnn', 'msnbc', 'npr', 'nytimes'), 'liberal', 
                           ifelse(experiment_group == 'usedgov', 'controlled', 'conservative')))
fc <- fc %>% group_by(politics) %>% 
  summarize(avg_fav = mean(favourites_count), agg_likes = sum(favourites_count), 
            avg_followers = mean(followers_count), agg_retweets = sum(followers_count))
fc
```

#### `verified` 
**Are there any verified accounts? If so, where did they engage with?**
None of the author is verified. 

```{r}
unique(final_tweets$verified)
```



\newpage
# EDA - with annotations

```{r}
annotated <- read.csv("data/master_annotated.csv")
glimpse(annotated)
```


```{r}
# split up (profile) created_at, today = 12/8/22
annotated <- annotated %>% mutate(age_dow = substr(created_at, 1, 3)
                              , age_month_day = substr(created_at, 5, 10)
                              , age_time = substr(created_at, 12, 19)
                              , age_yr = substr(created_at, 26, 30), 
                              , age_ymd = as.Date(paste0(age_month_day, age_yr), format = "%b %d %Y"),
                              , account_age = today - age_ymd)

annotated <- annotated %>% mutate(politics = ifelse(experiment_group %in% c('cnn', 'msnbc', 'npr', 'nytimes'), 'liberal', 
                           ifelse(experiment_group == 'usedgov', 'controlled', 'conservative')))
```

## `opinion_key` & `opinion_label`

41% of tweets are NEUTRAL in support of student loan forgiveness.29% are AGAINST, and 26% are FOR. Only 4% of the tweets are undetermined in sentiment. 
```{r}
annotated %>% group_by(opinion_label) %>% summarize(count = n(), proportion = n()/nrow(annotated))
```

Surprisingly, engagement with liberal has higher sentiment against student loan forgiveness (43%) compared to those engaging with FoxNews (27%). The conservative groups replies are mostly in the NEUTRAL support group (43%). **This raise the question of do people tend to reply to sources that they oppose (conservatives replying to liberal sources) or is there more dissent among those engaging with the liberal sources?** Both liberal and conservative sources have ~25% supportive replies for the forgiveness program. 

```{r echo=FALSE}
rbind(
  rbind(
            annotated %>% filter(politics == 'conservative') %>% 
              group_by(politics,opinion_label) %>% summarize(count = n()) %>% mutate(proportion = count/sum(count)),
            annotated %>% filter(politics == 'liberal') %>% 
              group_by(politics,opinion_label) %>% summarize(count = n()) %>% mutate(proportion = count/sum(count))
            ),
  annotated %>% filter(politics == 'controlled') %>% 
    group_by(politics,opinion_label) %>% summarize(count = n()) %>% mutate(proportion = count/sum(count))
  ) %>% arrange(politics, opinion_label)
```

Future expansion - grab the profile description of each author’s “friend/following” and create a threshold label on political affiliation based on verified profiles of those they follow. NLP through the profile description will also let us know if they are more left or right leaning. Currently, we cannot determine if the author political stand based on which news outlet they engage with on twitter (example @BUnskinkable appears to be more right leaning based on who he follows but he addressed @MSNBC)

```{r}
annotated %>% filter(screen_name == 'BUnskinkable') %>% select(experiment_group, text, tweet_id_char, screen_name)
```

Let's flip and look at "FOR student loan forgiveness" on the conservative side. 
@Richard41020: "@FoxNews Since I don't have any student loans I'd like for the government (Taxpayers), to payoff my mortgage.  Where do I sign up?"

Although it's categorized as FOR loan forgiveness, it is sarcasm. After exploring the profile, it is apparent that this author is conservative and does not support loan forgiveness. 

```{r}
annotated %>% filter(screen_name == 'Richard41020') %>% select(experiment_group, text, tweet_id_char, screen_name)
```

Let's choose another author. @TonyShockey6 is labeled as FOR forgiveness with .95 confidence, but it appears that his text does not support this annotation :( 
```{r}
annotated %>% filter(screen_name == 'TonyShockey6') %>% select(experiment_group, text, tweet_id_char, screen_name)

```

After further skimming, it appears that a lot of these FOR student loan forgiveness is categorized incorrectly based on the text provided by the annotators. 

```{r}
annotated %>% filter(politics == 'conservative', opinion_label == 'FOR student loan forgiveness ') %>% select('text')
```

**What is the `opinion_annotation_confidence` for each opinion and `politics` group? **

```{r}
annotated %>% group_by(politics, opinion_label) %>% summarize(AVG_opinion_annotation_confidence = mean(opinion_annotation_confidence))

```

## `ego_involvement_label`

75% of the tweets are from authors who find that student loan forgiveness issue is at least somewhat important. Only 15% have low ego involvement. 

```{r}
annotated %>% group_by(ego_involvement_label) %>% summarize(count = n(), proportion = n()/nrow(annotated))

```

## `opinion_annotation_confidence`

**What is the average of the confidence? What is the average of confidence of each annotated categories? What is the confidence for each news source? **

Our observations above that many of the text care incorrectly identified in opinion on student loan forgiveness program. We forgo answer the questions above and pivot to looking at how NLP and built in sentiment analysis fair against SageMaker's Mechanical Turks. 




# Excess 

## `ego_involvement_annotation_confidence`
*Give summary of the stand*

*What is the stand of the "older twitter accounts"?*

*Stacked bar plot on views on student loans and how it matches up against `experiment_group`*
ggplot(stacked_time, aes(fill=politics, y=Freq, x=hour)) + 
    geom_bar(position="stack", stat="identity")

**Is there an reason why someone who is against student loan forgiveness would reply to fox vs. cnn or vice versa?**
- refer to max likes, why is the opposition not addressing someone? 